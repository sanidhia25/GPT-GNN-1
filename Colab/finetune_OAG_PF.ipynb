{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"finetune_OAG_PF.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1kgfazQA4UEV1AiwfyXHSpSI1QG8VrrKT","authorship_tag":"ABX9TyP3Pb3V+zeb0T6UfWmFYL8b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"D9zIDag_b9-X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655978957798,"user_tz":-330,"elapsed":16217,"user":{"displayName":"Sanidhia Maheshwari","userId":"02437145800616888989"}},"outputId":"20cb6998-d02d-4327-f2d9-cb6c1bf0a099"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","!pip install transformers\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric\n","!pip install texttable"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lBpIY_kQcv40","executionInfo":{"status":"ok","timestamp":1655978997449,"user_tz":-330,"elapsed":34805,"user":{"displayName":"Sanidhia Maheshwari","userId":"02437145800616888989"}},"outputId":"a763007f-4df7-4cbf-f5f6-eb4f8257d9f4","collapsed":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 26.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 64.8 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 63.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 12.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n","\u001b[K     |████████████████████████████████| 7.9 MB 6.5 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.13\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 7.1 MB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n","\u001b[K     |████████████████████████████████| 750 kB 6.9 MB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n","\u001b[K     |████████████████████████████████| 407 kB 26.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=29595bba40259b065df37f3d149be6c3ab1ad65b5fdae3d767a6b1111e3ee066\n","  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n","Successfully built torch-geometric\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.0.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting texttable\n","  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n","Installing collected packages: texttable\n","Successfully installed texttable-1.6.4\n"]}]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/graph_CS.zip -d /content"],"metadata":{"id":"ETyIv_5HdKDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#add /content in the end of next 6 lines\n","!cp /content/drive/MyDrive/GPT_GNN/__init__.py\n","!cp /content/drive/MyDrive/GPT_GNN/conv.py\n","!cp /content/drive/MyDrive/GPT_GNN/model.py\n","!cp /content/drive/MyDrive/GPT_GNN/utils.py\n","!cp /content/drive/MyDrive/GPT_GNN/data.py\n","!cp -av /content/drive/MyDrive/GPT_GNN\n","import sys\n","from data import *\n","from model import *\n","from warnings import filterwarnings\n","filterwarnings(\"ignore\")"],"metadata":{"id":"n8JMTBk_cyTI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = \"/content/graph_CS.pk\"\n","use_pretrain = True\n","pretrain_model_dir = \"/content/drive/MyDrive/pretrain_model/model\"\n","model_dir = \"/content/drive/MyDrive/finetune_model\"\n","task_name = \"PF\"\n","cuda = 0\n","domain = \"_CS\"\n","sample_depth = 6\n","sample_width = 128\n","conv_name = \"hgt\"\n","n_hid = 400\n","n_heads=8\n","n_layers=3\n","prev_norm = True\n","last_norm = True\n","dropout = 0.2\n","optimizer = \"adamw\"\n","scheduler = \"cycle\"\n","data_percentage = 0.1\n","n_epoch = 1\n","n_pool = 2\n","n_batch = 16\n","batch_size = 256\n","clip = 0.5"],"metadata":{"id":"vvITmrBjdB9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if cuda != -1:\n","    device = torch.device(\"cuda:\" + str(cuda))\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print('Start Loading Graph Data...')\n","graph = renamed_load(open(os.path.join(data_dir, 'graph%s.pk' % domain), 'rb'))\n","print('Finish Loading Graph Data!')\n","\n","target_type = 'paper'\n","\n","types = graph.get_types()\n","'''\n","    cand_list stores all the L2 fields, which is the classification domain.\n","'''\n","cand_list = list(graph.edge_list['field']['paper']['PF_in_L2'].keys())\n","'''\n","Use KL Divergence here, since each paper can be associated with multiple fields.\n","Thus this task is a multi-label classification.\n","'''\n","criterion = nn.KLDivLoss(reduction='batchmean')"],"metadata":{"id":"sPgac0xdfkYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def node_classification_sample(seed, pairs, time_range):\n","    '''\n","        sub-graph sampling and label preparation for node classification:\n","        (1) Sample batch_size number of output nodes (papers), get their time.\n","    '''\n","    np.random.seed(seed)\n","    target_ids = np.random.choice(list(pairs.keys()), batch_size, replace = False)\n","    target_info = []\n","    for target_id in target_ids:\n","        _, _time = pairs[target_id]\n","        target_info += [[target_id, _time]]\n","    '''\n","        (2) Based on the seed nodes, sample a subgraph with 'sampled_depth' and 'sampled_number'\n","    '''\n","    feature, times, edge_list, _, _ = sample_subgraph(graph, time_range, \\\n","                inp = {'paper': np.array(target_info)}, \\\n","                sampled_depth = sample_depth, sampled_number = sample_width)\n","\n","    '''\n","        (3) Mask out the edge between the output target nodes (paper) with output source nodes (L2 field)\n","    '''\n","    masked_edge_list = []\n","    for i in edge_list['paper']['field']['rev_PF_in_L2']:\n","        if i[0] >= batch_size:\n","            masked_edge_list += [i]\n","    edge_list['paper']['field']['rev_PF_in_L2'] = masked_edge_list\n","\n","    masked_edge_list = []\n","    for i in edge_list['field']['paper']['PF_in_L2']:\n","        if i[1] >= batch_size:\n","            masked_edge_list += [i]\n","    edge_list['field']['paper']['PF_in_L2'] = masked_edge_list\n","    '''\n","        (4) Transform the subgraph into torch Tensor (edge_index is in format of pytorch_geometric)\n","    '''\n","    node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n","            to_torch(feature, times, edge_list, graph)\n","    '''\n","        (5) Prepare the labels for each output target node (paper), and their index in sampled graph.\n","            (node_dict[type][0] stores the start index of a specific type of nodes)\n","    '''\n","    ylabel = np.zeros([batch_size, len(cand_list)])\n","    for x_id, target_id in enumerate(target_ids):\n","        if target_id not in pairs:\n","            print('error 1' + str(target_id))\n","        for source_id in pairs[target_id][0]:\n","            if source_id not in cand_list:\n","                print('error 2' + str(target_id))\n","            ylabel[x_id][cand_list.index(source_id)] = 1\n","\n","    ylabel /= ylabel.sum(axis=1).reshape(-1, 1)\n","    x_ids = np.arange(batch_size) + node_dict['paper'][0]\n","    return node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel"],"metadata":{"id":"UVN9M4DZf08L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_data(pool):\n","    '''\n","        Sampled and prepare training and validation data using multi-process parallization.\n","    '''\n","    jobs = []\n","    for batch_id in np.arange(n_batch):\n","        p = pool.apply_async(node_classification_sample, args=(randint(), \\\n","            sel_train_pairs, train_range))\n","        jobs.append(p)\n","    p = pool.apply_async(node_classification_sample, args=(randint(), \\\n","            sel_valid_pairs, valid_range))\n","    jobs.append(p)\n","    return jobs"],"metadata":{"id":"MN5w2oqRgJLq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pre_range   = {t: True for t in graph.times if t != None and t < 2014}\n","train_range = {t: True for t in graph.times if t != None and t >= 2014  and t <= 2016}\n","valid_range = {t: True for t in graph.times if t != None and t > 2016  and t <= 2017}\n","test_range  = {t: True for t in graph.times if t != None and t > 2017}\n","\n","\n","train_pairs = {}\n","valid_pairs = {}\n","test_pairs  = {}\n","'''\n","    Prepare all the souce nodes (L2 field) associated with each target node (paper) as dict\n","'''\n","for target_id in graph.edge_list['paper']['field']['rev_PF_in_L2']:\n","    for source_id in graph.edge_list['paper']['field']['rev_PF_in_L2'][target_id]:\n","        _time = graph.edge_list['paper']['field']['rev_PF_in_L2'][target_id][source_id]\n","        if _time in train_range:\n","            if target_id not in train_pairs:\n","                train_pairs[target_id] = [[], _time]\n","            train_pairs[target_id][0] += [source_id]\n","        elif _time in valid_range:\n","            if target_id not in valid_pairs:\n","                valid_pairs[target_id] = [[], _time]\n","            valid_pairs[target_id][0] += [source_id]\n","        else:\n","            if target_id not in test_pairs:\n","                test_pairs[target_id]  = [[], _time]\n","            test_pairs[target_id][0]  += [source_id]\n","\n","\n","np.random.seed(43)\n","'''\n","    Only train and valid with a certain percentage of data, if necessary.\n","'''\n","sel_train_pairs = {p : train_pairs[p] for p in np.random.choice(list(train_pairs.keys()), int(len(train_pairs) * data_percentage), replace = False)}\n","sel_valid_pairs = {p : valid_pairs[p] for p in np.random.choice(list(valid_pairs.keys()), int(len(valid_pairs) * data_percentage), replace = False)}\n","\n","\n","\n","'''\n","    Initialize GNN (model is specified by conv_name) and Classifier\n","'''\n","gnn = GNN(conv_name = conv_name, in_dim = len(graph.node_feature[target_type]['emb'].values[0]) + 401, n_hid = n_hid, \\\n","          n_heads = n_heads, n_layers = n_layers, dropout = dropout, num_types = len(types), \\\n","          num_relations = len(graph.get_meta_graph()) + 1, prev_norm = prev_norm, last_norm = last_norm)\n","if use_pretrain:\n","    gnn.load_state_dict(load_gnn(torch.load(pretrain_model_dir)), strict = False)\n","    print('Load Pre-trained Model from (%s)' % pretrain_model_dir)\n","classifier = Classifier(n_hid, len(cand_list))\n","\n","model = nn.Sequential(gnn, classifier).to(device)\n","\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-4)\n","\n","stats = []\n","res = []\n","best_val   = 0\n","train_step = 0\n","\n","pool = mp.Pool(n_pool)\n","st = time.time()\n","jobs = prepare_data(pool)\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 500, eta_min=1e-6)"],"metadata":{"id":"R5R1wpjCgQyN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in np.arange(n_epoch) + 1:\n","    '''\n","        Prepare Training and Validation Data\n","    '''\n","    train_data = [job.get() for job in jobs[:-1]]\n","    valid_data = jobs[-1].get()\n","    pool.close()\n","    pool.join()\n","    '''\n","        After the data is collected, close the pool and then reopen it.\n","    '''\n","    pool = mp.Pool(n_pool)\n","    jobs = prepare_data(pool)\n","    et = time.time()\n","    print('Data Preparation: %.1fs' % (et - st))\n","\n","    '''\n","        Train (2014 <= time <= 2016)\n","    '''\n","    model.train()\n","    train_losses = []\n","    for node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel in train_data:\n","        node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n","                               edge_time.to(device), edge_index.to(device), edge_type.to(device))\n","        res  = classifier.forward(node_rep[x_ids])\n","        loss = criterion(res, torch.FloatTensor(ylabel).to(device))\n","\n","        optimizer.zero_grad() \n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","\n","        train_losses += [loss.cpu().detach().tolist()]\n","        train_step += 1\n","        scheduler.step(train_step)\n","        del res, loss\n","    '''\n","        Valid (2017 <= time <= 2017)\n","    '''\n","    model.eval()\n","    with torch.no_grad():\n","        node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel = valid_data\n","        node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n","                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n","        res  = classifier.forward(node_rep[x_ids])\n","        loss = criterion(res, torch.FloatTensor(ylabel).to(device))\n","\n","        '''\n","            Calculate Valid NDCG. Update the best model based on highest NDCG score.\n","        '''\n","        valid_res = []\n","        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n","            valid_res += [ai[bi.cpu().numpy()]]\n","        valid_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res])\n","        if valid_ndcg > best_val:\n","            best_val = valid_ndcg\n","            torch.save(model, os.path.join(model_dir, task_name + '_' + conv_name))\n","            print('UPDATE!!!')\n","\n","        st = time.time()\n","        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %.2f  Valid Loss: %.2f  Valid NDCG: %.4f\") % \\\n","              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), \\\n","                    loss.cpu().detach().tolist(), valid_ndcg))\n","        stats += [[np.average(train_losses), loss.cpu().detach().tolist()]]\n","        del res, loss\n","    del train_data, valid_data"],"metadata":{"id":"z0H6exXFg1j-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","    Evaluate the trained model via test set (time >= 2018)\n","'''\n","\n","\n","best_model = torch.load(os.path.join(model_dir, task_name + '_' + conv_name))\n","best_model.eval()\n","gnn, classifier = best_model\n","with torch.no_grad():\n","    test_res = []\n","    for _ in range(10):\n","        node_feature, node_type, edge_time, edge_index, edge_type, x_ids, ylabel = \\\n","                    node_classification_sample(randint(), test_pairs, test_range)\n","        paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n","                    edge_time.to(device), edge_index.to(device), edge_type.to(device))[x_ids]\n","        res = classifier.forward(paper_rep)\n","        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n","            test_res += [ai[bi.cpu().numpy()]]\n","    test_ndcg = [ndcg_at_k(resi, len(resi)) for resi in test_res]\n","    print('Best Test NDCG: %.4f' % np.average(test_ndcg))\n","    test_mrr = mean_reciprocal_rank(test_res)\n","    print('Best Test MRR:  %.4f' % np.average(test_mrr))\n"],"metadata":{"id":"VVsEgmjGhOyW"},"execution_count":null,"outputs":[]}]}