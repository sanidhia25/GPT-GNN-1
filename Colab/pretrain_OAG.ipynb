{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pretrain_OAG.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNtfMxNhQ5vNrqESpZm41CZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uA5ZKAOMXDVI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","!pip install transformers\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric\n","!pip install texttable"],"metadata":{"id":"W90Tx4gMXNSa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/graph_CS.zip -d /content"],"metadata":{"id":"n9y73EJbXPsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#add /content in the end of next 6 lines\n","!cp /content/drive/MyDrive/GPT_GNN/__init__.py\n","!cp /content/drive/MyDrive/GPT_GNN/conv.py\n","!cp /content/drive/MyDrive/GPT_GNN/model.py\n","!cp /content/drive/MyDrive/GPT_GNN/utils.py\n","!cp /content/drive/MyDrive/GPT_GNN/data.py\n","!cp -av /content/drive/MyDrive/GPT_GNN\n","import sys\n","from data import *\n","from model import *\n","from warnings import filterwarnings"],"metadata":{"id":"NnLOZP3UX4lE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attr_ratio = 0.5\n","attr_type = 'text'\n","neg_samp_num = 255\n","queue_size = 256\n","w2v_dir = '/content/drive/MyDrive/w2v_all'\n","data_dir = '/content/graph_CS.pk'\n","pretrain_model_dir = '/content/drive/MyDrive/pretrain_model/model'\n","cuda = 0\n","sample_depth = 6\n","sample_width = 128\n","conv_name = 'hgt'\n","n_hid = 400\n","n_heads = 8\n","n_layers = 3\n","prev_norm = True\n","last_norm = True\n","dropout = 0.2\n","max_lr = 1e-3\n","scheduler = 'cycle'\n","n_epoch = 2\n","n_pool = 1\n","n_batch = 32\n","batch_size = 256\n","clip = 0.5"],"metadata":{"id":"n-rc8_7GXYYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if cuda != -1:\n","    device = torch.device(\"cuda:\" + str(cuda))\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print('Start Loading Graph Data...')\n","graph = renamed_load(open(data_dir, 'rb'))\n","print('Finish Loading Graph Data!')\n","\n","pre_range   = {t: True for t in graph.times if t != None and t < 2014}\n","train_range = {t: True for t in graph.times if t != None and t >= 2014  and t <= 2016}\n","valid_range = {t: True for t in graph.times if t != None and t > 2016  and t <= 2017}\n","test_range  = {t: True for t in graph.times if t != None and t > 2017}\n","\n","pre_target_nodes   = []\n","train_target_nodes = []\n","target_type = 'paper'\n","rel_stop_list = ['self', 'rev_PF_in_L0', 'rev_PF_in_L5', 'rev_PV_Repository', 'rev_PV_Patent']\n","\n","for p_id, _time in graph.node_feature[target_type]['time'].iteritems():\n","    if _time in pre_range:\n","        pre_target_nodes += [[p_id, _time]]\n","    elif _time in train_range:\n","        train_target_nodes += [[p_id, _time]]\n","pre_target_nodes = np.array(pre_target_nodes)\n","train_target_nodes = np.array(train_target_nodes)"],"metadata":{"id":"3VTf2_53XjiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def GPT_sample(seed, target_nodes, time_range, batch_size, feature_extractor):\n","    np.random.seed(seed)\n","    samp_target_nodes = target_nodes[np.random.choice(len(target_nodes), batch_size)]\n","    threshold   = 0.5\n","    feature, times, edge_list, _, attr = sample_subgraph(graph, time_range, \\\n","                inp = {target_type: samp_target_nodes}, feature_extractor = feature_extractor, \\\n","                    sampled_depth = sample_depth, sampled_number = sample_width)\n","    rem_edge_list = defaultdict(  #source_type\n","                        lambda: defaultdict(  #relation_type\n","                            lambda: [] # [target_id, source_id] \n","                                ))\n","    \n","    ori_list = {}\n","    for source_type in edge_list[target_type]:\n","        ori_list[source_type] = {}\n","        for relation_type in edge_list[target_type][source_type]:\n","            ori_list[source_type][relation_type] = np.array(edge_list[target_type][source_type][relation_type])\n","            el = []\n","            for target_ser, source_ser in edge_list[target_type][source_type][relation_type]:\n","                if relation_type not in rel_stop_list and target_ser < batch_size and np.random.random() > threshold:\n","                    rem_edge_list[source_type][relation_type] += [[target_ser, source_ser]]\n","                    continue\n","                el += [[target_ser, source_ser]]\n","            el = np.array(el)\n","            edge_list[target_type][source_type][relation_type] = el\n","            \n","            if relation_type == 'self':\n","                continue\n","            else:\n","                if 'rev_' in relation_type:\n","                    rev_relation = relation_type[4:]\n","                else:\n","                    rev_relation = 'rev_' + relation_type\n","                edge_list[source_type]['paper'][rev_relation] = list(np.stack((el[:,1], el[:,0])).T)\n","                \n","    '''\n","        Adding feature nodes:\n","    '''\n","    n_target_nodes = len(feature[target_type])\n","    feature[target_type] = np.concatenate((feature[target_type], np.zeros([batch_size, feature[target_type].shape[1]])))\n","    times[target_type]   = np.concatenate((times[target_type], times[target_type][:batch_size]))\n","\n","    for source_type in edge_list[target_type]:\n","        for relation_type in edge_list[target_type][source_type]:\n","            el = []\n","            for target_ser, source_ser in edge_list[target_type][source_type][relation_type]:\n","                if target_ser < batch_size:\n","                    if relation_type == 'self':\n","                        el += [[target_ser + n_target_nodes, target_ser + n_target_nodes]]\n","                    else:\n","                        el += [[target_ser + n_target_nodes, source_ser]]\n","            if len(el) > 0:\n","                edge_list[target_type][source_type][relation_type] = \\\n","                    np.concatenate((edge_list[target_type][source_type][relation_type], el))\n","\n","\n","    rem_edge_lists = {}\n","    for source_type in rem_edge_list:\n","        rem_edge_lists[source_type] = {}\n","        for relation_type in rem_edge_list[source_type]:\n","            rem_edge_lists[source_type][relation_type] = np.array(rem_edge_list[source_type][relation_type])\n","    del rem_edge_list\n","          \n","    return to_torch(feature, times, edge_list, graph), rem_edge_lists, ori_list, \\\n","            attr[:batch_size], (n_target_nodes, n_target_nodes + batch_size)\n"],"metadata":{"id":"BswOIcNnXmzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_data(pool):\n","    jobs = []\n","    for _ in np.arange(n_batch - 1):\n","        jobs.append(pool.apply_async(GPT_sample, args=(randint(), pre_target_nodes, pre_range, batch_size, feature_OAG)))\n","    jobs.append(pool.apply_async(GPT_sample, args=(randint(), train_target_nodes, train_range, batch_size, feature_OAG)))\n","    return jobs"],"metadata":{"id":"w-VGeOzFXpvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pool = mp.Pool(n_pool)\n","st = time.time()\n","jobs = prepare_data(pool)\n","repeat_num = int(len(pre_target_nodes) / batch_size // n_batch)\n","\n","\n","data, rem_edge_list, ori_edge_list, _, _ = GPT_sample(randint(), pre_target_nodes, pre_range, batch_size, feature_OAG)\n","node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = data\n","types = graph.get_types()\n","\n","\n","gnn = GNN(conv_name = conv_name, in_dim = len(graph.node_feature[target_type]['emb'].values[0]) + 401, n_hid = n_hid, \\\n","          n_heads = n_heads, n_layers = n_layers, dropout = dropout, num_types = len(types), \\\n","          num_relations = len(graph.get_meta_graph()) + 1, prev_norm = prev_norm, last_norm = last_norm)"],"metadata":{"id":"HVMFkhrLXsMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if attr_type == 'text':  \n","    from gensim.models import Word2Vec\n","    w2v_model = Word2Vec.load(w2v_dir)\n","    n_tokens = len(w2v_model.wv.vocab)\n","    attr_decoder = RNNModel(n_word = n_tokens, ninp = gnn.n_hid, \\\n","               nhid = w2v_model.vector_size, nlayers = 2)\n","    attr_decoder.from_w2v(torch.FloatTensor(w2v_model.wv.vectors))\n","else:\n","    attr_decoder = Matcher(gnn.n_hid, gnn.in_dim)\n","    \n","gpt_gnn = GPT_GNN(gnn = gnn, rem_edge_list = rem_edge_list, attr_decoder = attr_decoder, \\\n","                  neg_queue_size = 0, types = types, neg_samp_num = neg_samp_num, device = device)\n","gpt_gnn.init_emb.data = node_feature[node_type == node_dict[target_type][1]].mean(dim=0).detach()\n","gpt_gnn = gpt_gnn.to(device)\n","\n","best_val   = 100000\n","train_step = 0\n","stats = []\n","optimizer = torch.optim.AdamW(gpt_gnn.parameters(), weight_decay = 1e-2, eps=1e-06, lr = max_lr)\n","\n","if scheduler == 'cycle':\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, pct_start=0.02, anneal_strategy='linear', final_div_factor=100,\\\n","                        max_lr = max_lr, total_steps = repeat_num * n_batch * n_epoch + 1)\n","elif scheduler == 'cosine':\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, repeat_num * n_batch, eta_min=1e-6)"],"metadata":{"id":"CNNNhD2yXwM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Start Pretraining...')\n","for epoch in np.arange(n_epoch) + 1:\n","    gpt_gnn.neg_queue_size = queue_size * epoch // n_epoch\n","    for batch in np.arange(repeat_num) + 1:\n","        train_data = [job.get() for job in jobs[:-1]]\n","        valid_data = jobs[-1].get()\n","        pool.close()\n","        pool.join()\n","        pool = mp.Pool(n_pool)\n","        jobs = prepare_data(pool)\n","        et = time.time()\n","        print('Data Preparation: %.1fs' % (et - st))\n","\n","        train_link_losses = []\n","        train_attr_losses = []\n","        gpt_gnn.train()\n","        for data, rem_edge_list, ori_edge_list, attr, (start_idx, end_idx) in train_data:\n","            node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = data\n","            node_feature = node_feature.detach()\n","            node_feature[start_idx : end_idx] = gpt_gnn.init_emb\n","            node_emb = gpt_gnn.gnn(node_feature.to(device), node_type.to(device), edge_time.to(device), \\\n","                                   edge_index.to(device), edge_type.to(device))\n","\n","            loss_link, _ = gpt_gnn.link_loss(node_emb, rem_edge_list, ori_edge_list, node_dict, target_type, use_queue = True, update_queue=True)\n","            if attr_type == 'text':\n","                loss_attr = gpt_gnn.text_loss(node_emb[start_idx : end_idx], attr, w2v_model, device)\n","            else:\n","                loss_attr = gpt_gnn.feat_loss(node_emb[start_idx : end_idx], torch.FloatTensor(attr).to(device))\n","\n","\n","            loss = loss_link + loss_attr * attr_ratio\n","\n","\n","            optimizer.zero_grad() \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(gpt_gnn.parameters(), clip)\n","            optimizer.step()\n","\n","            train_link_losses += [loss_link.item()]\n","            train_attr_losses += [loss_attr.item()]\n","            scheduler.step()\n","        '''\n","            Valid\n","        '''\n","        gpt_gnn.eval()\n","        with torch.no_grad():\n","            data, rem_edge_list, ori_edge_list, attr, (start_idx, end_idx) = valid_data\n","            node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = data\n","            node_feature = node_feature.detach()\n","            node_feature[start_idx : end_idx] = gpt_gnn.init_emb\n","            node_emb = gpt_gnn.gnn(node_feature.to(device), node_type.to(device), edge_time.to(device), \\\n","                                       edge_index.to(device), edge_type.to(device))\n","            loss_link, ress = gpt_gnn.link_loss(node_emb, rem_edge_list, ori_edge_list, node_dict, target_type, use_queue = False, update_queue=True)\n","            loss_link = loss_link.item()\n","            if attr_type == 'text':   \n","                loss_attr = gpt_gnn.text_loss(node_emb[start_idx : end_idx], attr, w2v_model, device)\n","            else:\n","                loss_attr = gpt_gnn.feat_loss(node_emb[start_idx : end_idx], torch.FloatTensor(attr).to(device))\n","\n","            ndcgs = []\n","            for i in ress:\n","                ai = np.zeros(len(i[0]))\n","                ai[0] = 1\n","                ndcgs += [ndcg_at_k(ai[j.cpu().numpy()], len(j)) for j in i.argsort(descending = True)]     \n","                \n","            valid_loss = loss_link + loss_attr * attr_ratio\n","            st = time.time()\n","            print((\"Epoch: %d, (%d / %d) %.1fs  LR: %.5f Train Loss: (%.3f, %.3f)  Valid Loss: (%.3f, %.3f)  NDCG: %.3f  Norm: %.3f  queue: %d\") % \\\n","                  (epoch, batch, repeat_num, (st-et), optimizer.param_groups[0]['lr'], np.average(train_link_losses), np.average(train_attr_losses), \\\n","                   loss_link, loss_attr, np.average(ndcgs), node_emb.norm(dim=1).mean(), gpt_gnn.neg_queue_size))  \n","            \n","        if valid_loss < best_val:\n","            best_val = valid_loss\n","            print('UPDATE!!!')\n","            torch.save(gpt_gnn.state_dict(), pretrain_model_dir)\n","        stats += [[np.average(train_link_losses),  loss_link, loss_attr, valid_loss]]\n"],"metadata":{"id":"efWcdVpxXylj"},"execution_count":null,"outputs":[]}]}